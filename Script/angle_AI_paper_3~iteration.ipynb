{
  "cells": [
  ---
**Code adapté de :** [AnglE Quickstart](https://angle.readthedocs.io/en/latest/notes/quickstart.html)  
**Référence de l'article :**  
Li, Xianming, et Jing Li. *"AnglE-Optimized Text Embeddings."*  
arXiv preprint arXiv:2309.12871 (2023).
---
]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59740d1c-5aa4-42b8-b544-fb5f8449c042",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59740d1c-5aa4-42b8-b544-fb5f8449c042",
        "outputId": "c71b6660-6a44-4f4b-90ad-f3e6768aec66"
      },
      "outputs": [],
      "source": [
        "!python -m pip install -U angle-emb billm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7417848-f6d6-407f-bd7f-bce4323aa578",
      "metadata": {
        "id": "e7417848-f6d6-407f-bd7f-bce4323aa578"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from angle_emb import AnglE\n",
        "from angle_emb.utils import cosine_similarity\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b4935d-884a-4bad-862a-ca6130549ac3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9b4935d-884a-4bad-862a-ca6130549ac3",
        "outputId": "6fb7c7b8-9666-4c5d-eb62-88386743e119"
      },
      "outputs": [],
      "source": [
        "#Nous faisons juste un test pour voir si tout est bien installé\n",
        "\n",
        "\n",
        "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
        "# for non-retrieval tasks, we don't need to specify prompt when using UAE-Large-V1.\n",
        "doc_vecs = angle.encode([\n",
        "    'AI developers and researchers focusing on anomaly detection algorithms for non-intrusive inspection systems.',\n",
        "    'Smugglers'\n",
        "])\n",
        "\n",
        "for i, dv1 in enumerate(doc_vecs):\n",
        "    for dv2 in doc_vecs[i+1:]:\n",
        "        print(cosine_similarity(dv1, dv2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863527fc-b7f3-41ff-a4cb-b70188048def",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "863527fc-b7f3-41ff-a4cb-b70188048def",
        "outputId": "5826c07e-611b-479e-b1fc-2f3def349c7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "###################################### llama-3/human\n",
        "\n",
        "# Lire les données depuis le fichier Excel\n",
        "df = pd.read_excel('/2023 fichier de base Copie.xlsx')\n",
        "df.iloc[:, 12] = df.iloc[:, 12].fillna('neutral')\n",
        "# Assurez-vous que les colonnes 10 et 11 existent et contiennent les textes\n",
        "col10_texts = df.iloc[:, 11].dropna().tolist()  # Target population using Llama-3\n",
        "col11_texts = df.iloc[:, 12].dropna().tolist()  # Target population using llm and human review\n",
        "\n",
        "\n",
        "# Vérifiez que les deux listes ont la même longueur\n",
        "if len(col10_texts) != len(col11_texts):\n",
        "    raise ValueError(\"Les colonnes 10 et 11 doivent avoir le même nombre de textes.\")\n",
        "\n",
        "# Instancier le modèle AnglE\n",
        "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
        "\n",
        "# Encoder les textes des deux colonnes\n",
        "doc_vecs_col10 = angle.encode(col10_texts)\n",
        "doc_vecs_col11 = angle.encode(col11_texts)\n",
        "\n",
        "similarities = [cosine_similarity(dv1, dv2) for dv1, dv2 in zip(doc_vecs_col10, doc_vecs_col11)]\n",
        "\n",
        "# Ajouter les résultats au DataFrame dans la colonne 15 (index 14)\n",
        "df.insert(13, 'Similarité Cosinus llama-3/human m1', pd.Series(similarities))\n",
        "\n",
        "df.to_excel('/2023 fichier de base Copie.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nu8HQmeXs7Fa",
      "metadata": {
        "id": "nu8HQmeXs7Fa"
      },
      "outputs": [],
      "source": [
        "###################################### Qwen-2/human\n",
        "\n",
        "\n",
        "# Lire les données depuis le fichier Excel\n",
        "df = pd.read_excel('/2023 fichier de base Copie.xlsx')\n",
        "df.iloc[:, 12] = df.iloc[:, 12].fillna('neutral')\n",
        "\n",
        "# Assurez-vous que les colonnes 10 et 11 existent et contiennent les textes\n",
        "col10_texts = df.iloc[:, 10].dropna().tolist()  # Target population using Qwen-2\n",
        "col11_texts = df.iloc[:, 12].dropna().tolist()  # Target population using llm and human review\n",
        "\n",
        "# Vérifiez que les deux listes ont la même longueur\n",
        "if len(col10_texts) != len(col11_texts):\n",
        "    raise ValueError(\"Les colonnes 10 et 11 doivent avoir le même nombre de textes.\")\n",
        "\n",
        "# Instancier le modèle AnglE\n",
        "angle = AnglE.from_pretrained('WhereIsAI/UAE-Large-V1', pooling_strategy='cls').cuda()\n",
        "\n",
        "# Définir une taille de batch en fonction de la mémoire disponible\n",
        "batch_size = 64  # Essayez d'augmenter ou de réduire cette valeur selon la mémoire disponible\n",
        "\n",
        "# Fonction pour encoder en batch\n",
        "def encode_in_batches(texts, model, batch_size):\n",
        "    dataloader = DataLoader(texts, batch_size=batch_size, shuffle=False)\n",
        "    all_encodings = []\n",
        "    for batch in dataloader:\n",
        "        encodings = model.encode(batch)\n",
        "        all_encodings.extend(encodings)\n",
        "        torch.cuda.empty_cache()  # Libérer la mémoire GPU après chaque batch\n",
        "    return all_encodings\n",
        "\n",
        "# Encoder les textes des deux colonnes en batches\n",
        "doc_vecs_col10 = encode_in_batches(col10_texts, angle, batch_size)\n",
        "doc_vecs_col11 = encode_in_batches(col11_texts, angle, batch_size)\n",
        "\n",
        "# Calculer les similarités cosinus\n",
        "similarities = [cosine_similarity([dv1], [dv2])[0][0] for dv1, dv2 in zip(doc_vecs_col10, doc_vecs_col11)]\n",
        "\n",
        "# Ajouter les résultats au DataFrame dans la colonne 15 (index 14)\n",
        "df.insert(14, 'Similarité Cosinus Qwen2/human m1', pd.Series(similarities))\n",
        "\n",
        "# Sauvegarder les résultats dans le fichier Excel\n",
        "df.to_excel('/2023 fichier de base Copie.xlsx', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
